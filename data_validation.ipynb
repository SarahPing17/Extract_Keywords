{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 142\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a helpful assistant who will help me to summarize the keywords from the given research papers.'}\n",
      "{'role': 'user', 'content': 'Presley\\xa0 et\\xa0al. \\nPhilosophy, Ethics, and Humanities in Medicine            (2022) 17:8  \\nhttps://doi.org/10.1186/s13010-022-00119-z\\nRESEARCH\\nMedia portrayal of\\xa0ethical and\\xa0social issues \\nin\\xa0brain organoid research\\nAbigail Presley1, Leigh Ann Samsa2 and Veljko Dubljević1,3*  \\nAbstract \\nBackground: Human brain organoids are a valuable research tool for studying brain development, physiology, and \\npathology. Yet, a host of potential ethical concerns are inherent in their creation. There is a growing group of bioethi-\\ncists who acknowledge the moral imperative to develop brain organoid technologies and call for caution in this \\nresearch. Although a relatively new technology, brain organoids and their uses are already being discussed in media \\nliterature. Media literature informs the public and policymakers but has the potential for utopian or dystopian distor -\\ntions. Thus, it is important to understand how this technology is portrayed to the public.\\nMethods: To investigate how brain organoids are displayed to the public, we conducted a systematic review of \\nmedia literature indexed in the Nexis Uni database from 2013–2019. News and media source articles passing exclu-\\nsion criteria (n = 93) were scored to evaluate tone and relevant themes. Themes were validated with a pilot sample \\nbefore being applied to the dataset. Thematic analysis assessed article tone, reported potential for the technology, \\nand the scientific, social, and ethical contexts surrounding brain organoids research.\\nResults: Brain organoid publications became more frequent from 2013 to 2019. We observed increases in positively \\nand negatively toned articles, suggesting growing polarization. While many sources discuss realistic applications of \\nbrain organoids, others suggest treatment and cures beyond the scope of the current technology. This could work \\nto overhype the technology and disillusion patients and families by offering false hope. In the ethical narrative we \\nobserve a preoccupation with issues such as development of artificial consciousness and “humanization” of organoid-\\nanimal chimeras. Issues of regulation, ownership, and accuracy of the organoid models are rarely discussed.\\nConclusions: Given the power that media have to inform or misinform the public, it is important this literature \\nprovides an accurate and balanced reflection of the therapeutic potential and associated ethical issues regarding \\nbrain organoid research. Our study suggests increasing polarization, coupled with misplaced and unfounded ethical \\nconcern. Given the inhibitory effects of public fear or disillusion on research funding, it is important media literature \\nprovides an accurate reflection of brain organoids.\\nKeywords: Brain organoid, Cerebroid, Bioethics, Neuroethics, Media\\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \\nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \\nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \\nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \\nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco  \\nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.Introduction\\nGrowth of science and technology can lead to public \\ninspiration or dread. Scientists can now coax stem cells \\nin culture to grow into miniature tissues called organoids, which are designed to recapitulate organs in\\xa0 vitro, and \\ndeveloped from pluripotent stem cells to exhibit multi-\\ncell differentiation and self-organization (Lancaster and \\nKnoblich 2014). These small, complex 3D structures \\ncontain cells of the tissue of origin and are used to study \\ndevelopment, physiology, and disease. The first big break -\\nthrough in organoid biotechnologies was reported more \\nthan a decade ago in 2009, when Hans Clevers’ group \\ncreated mouse intestinal epithelial organoids (Sato et\\xa0al. Open Access\\n*Correspondence:  veljko_dubljevic@ncsu.edu\\n3 Department of Philosophy and Religious studies, NC State University, \\n101 Lampe Drive, Withers Hall 453, 27695 Raleigh, USA\\nFull list of author information is available at the end of the article'}\n",
      "{'role': 'assistant', 'content': 'Brain organoid;Cerebroid;Bioethics;Neuroethics;Media'}\n"
     ]
    }
   ],
   "source": [
    "# data_path = \"/Users/yihanping/Documents/gatech/Research/data_prep/ProcessPDF/pdf_downloads/processed_json/CS/messages.jsonl\"\n",
    "data_path = \"/Users/yihanping/Documents/gatech/Research/openai/AllMajor_messages_large.jsonl\"\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 639, 1262\n",
      "mean / median: 955.0492957746479, 980.0\n",
      "p5 / p95: 757.1, 1126.5\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 10, 37\n",
      "mean / median: 19.816901408450704, 19.0\n",
      "p5 / p95: 13.0, 28.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~73463 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~220389 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
